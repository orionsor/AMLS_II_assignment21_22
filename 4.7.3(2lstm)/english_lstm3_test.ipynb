{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168a2ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from collections import Counter \n",
    "import itertools\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c3cb8",
   "metadata": {},
   "source": [
    "# data import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba16870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>681877834982232064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ShaquilleHoNeal from what I think you're aski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>681879579129200640</td>\n",
       "      <td>positive</td>\n",
       "      <td>Iran ranks 1st in liver surgeries, Allah bless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>681883903259357184</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Hours before he arrived in Saudi Arabia on Tue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>681904976860327936</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VanityFair  Alex Kim Kardashian worth how to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>681910549211287552</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I guess even Pandora knows Justin Bieber is a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20632 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id sentiment  \\\n",
       "0      619950566786113536   neutral   \n",
       "1      619969366986235905   neutral   \n",
       "2      619971047195045888  negative   \n",
       "3      619974445185302528   neutral   \n",
       "4      619987808317407232  positive   \n",
       "...                   ...       ...   \n",
       "20627  681877834982232064   neutral   \n",
       "20628  681879579129200640  positive   \n",
       "20629  681883903259357184   neutral   \n",
       "20630  681904976860327936  negative   \n",
       "20631  681910549211287552   neutral   \n",
       "\n",
       "                                                   tweet  \n",
       "0      Picturehouse's, Pink Floyd's, 'Roger Waters: T...  \n",
       "1      Order Go Set a Watchman in store or through ou...  \n",
       "2      If these runway renovations at the airport pre...  \n",
       "3      If you could ask an onstage interview question...  \n",
       "4      A portion of book sales from our Harper Lee/Go...  \n",
       "...                                                  ...  \n",
       "20627  @ShaquilleHoNeal from what I think you're aski...  \n",
       "20628  Iran ranks 1st in liver surgeries, Allah bless...  \n",
       "20629  Hours before he arrived in Saudi Arabia on Tue...  \n",
       "20630  @VanityFair  Alex Kim Kardashian worth how to ...  \n",
       "20631  I guess even Pandora knows Justin Bieber is a ...  \n",
       "\n",
       "[20632 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table('./data/twitter-2016test-A.txt' , usecols=[0,1,2], encoding='utf-8', names=['id','sentiment', 'tweet'])\n",
    "#dataTest = pd.read_table('../input/semevalll/SemEval2017-test.txt', usecols=[1,2], encoding='utf-8', names=['sentiment', 'tweet'])                   \n",
    "#combine = [dataTrain,dataTest]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617906fe",
   "metadata": {},
   "source": [
    "# add label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b3fdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORY_INDEX = {\n",
    "    \"negative\": -1,\n",
    "    \"neutral\": 0,\n",
    "    \"positive\": 1\n",
    "}\n",
    "\n",
    "\"\"\"import data \"\"\"\n",
    "raw_label = df['sentiment'].values.tolist()\n",
    "rawlabel = []\n",
    "for i in range(len(raw_label)):\n",
    "    rawlabel.append(CATEGORY_INDEX[raw_label[i]])\n",
    "rawlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a800c715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>681877834982232064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ShaquilleHoNeal from what I think you're aski...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>681879579129200640</td>\n",
       "      <td>positive</td>\n",
       "      <td>Iran ranks 1st in liver surgeries, Allah bless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>681883903259357184</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Hours before he arrived in Saudi Arabia on Tue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>681904976860327936</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VanityFair  Alex Kim Kardashian worth how to ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>681910549211287552</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I guess even Pandora knows Justin Bieber is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20632 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id sentiment  \\\n",
       "0      619950566786113536   neutral   \n",
       "1      619969366986235905   neutral   \n",
       "2      619971047195045888  negative   \n",
       "3      619974445185302528   neutral   \n",
       "4      619987808317407232  positive   \n",
       "...                   ...       ...   \n",
       "20627  681877834982232064   neutral   \n",
       "20628  681879579129200640  positive   \n",
       "20629  681883903259357184   neutral   \n",
       "20630  681904976860327936  negative   \n",
       "20631  681910549211287552   neutral   \n",
       "\n",
       "                                                   tweet  label  \n",
       "0      Picturehouse's, Pink Floyd's, 'Roger Waters: T...      0  \n",
       "1      Order Go Set a Watchman in store or through ou...      0  \n",
       "2      If these runway renovations at the airport pre...     -1  \n",
       "3      If you could ask an onstage interview question...      0  \n",
       "4      A portion of book sales from our Harper Lee/Go...      1  \n",
       "...                                                  ...    ...  \n",
       "20627  @ShaquilleHoNeal from what I think you're aski...      0  \n",
       "20628  Iran ranks 1st in liver surgeries, Allah bless...      1  \n",
       "20629  Hours before he arrived in Saudi Arabia on Tue...      0  \n",
       "20630  @VanityFair  Alex Kim Kardashian worth how to ...     -1  \n",
       "20631  I guess even Pandora knows Justin Bieber is a ...      0  \n",
       "\n",
       "[20632 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = rawlabel\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381a0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>681877834982232064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ShaquilleHoNeal from what I think you're aski...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>681879579129200640</td>\n",
       "      <td>positive</td>\n",
       "      <td>Iran ranks 1st in liver surgeries, Allah bless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>681883903259357184</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Hours before he arrived in Saudi Arabia on Tue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>681904976860327936</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VanityFair  Alex Kim Kardashian worth how to ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>681910549211287552</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I guess even Pandora knows Justin Bieber is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20632 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id sentiment  \\\n",
       "0      619950566786113536   neutral   \n",
       "1      619969366986235905   neutral   \n",
       "2      619971047195045888  negative   \n",
       "3      619974445185302528   neutral   \n",
       "4      619987808317407232  positive   \n",
       "...                   ...       ...   \n",
       "20627  681877834982232064   neutral   \n",
       "20628  681879579129200640  positive   \n",
       "20629  681883903259357184   neutral   \n",
       "20630  681904976860327936  negative   \n",
       "20631  681910549211287552   neutral   \n",
       "\n",
       "                                                   tweet  label  \n",
       "0      Picturehouse's, Pink Floyd's, 'Roger Waters: T...      0  \n",
       "1      Order Go Set a Watchman in store or through ou...      0  \n",
       "2      If these runway renovations at the airport pre...     -1  \n",
       "3      If you could ask an onstage interview question...      0  \n",
       "4      A portion of book sales from our Harper Lee/Go...      1  \n",
       "...                                                  ...    ...  \n",
       "20627  @ShaquilleHoNeal from what I think you're aski...      0  \n",
       "20628  Iran ranks 1st in liver surgeries, Allah bless...      1  \n",
       "20629  Hours before he arrived in Saudi Arabia on Tue...      0  \n",
       "20630  @VanityFair  Alex Kim Kardashian worth how to ...     -1  \n",
       "20631  I guess even Pandora knows Justin Bieber is a ...      0  \n",
       "\n",
       "[20632 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_back = df.copy()\n",
    "df_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d575a7",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1937e0c",
   "metadata": {},
   "source": [
    "## 1 data cleansing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610b62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0902c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(tweets, clean_object):\n",
    "        #tweets.loc[:, \"tweet\"].replace(clean_object, \"\", inplace=True)\n",
    "        tweets = re.sub(clean_object, ' ', tweets)\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74b97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(tweets):\n",
    "        return clean_base(tweets, re.compile(r\"http.?://[^\\s]+[\\s]?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dfbdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames(tweets):\n",
    "        return clean_base(tweets, re.compile(r\"@[^\\s]+[\\s]?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090412aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(tweets):  # it unrolls the hashtags to normal words\n",
    "        for hashtag in map(lambda x: re.compile(re.escape(x)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\"]):\n",
    "            tweets = re.sub(hashtag, ' ', tweets)\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea79e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(tweets):\n",
    "        return clean_base(tweets, re.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe15fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6010822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbbf0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d63f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de7fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouses Pink Floyds Roger Waters The Wal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper LeeGo ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>681877834982232064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>from what I think youre asking in no order Fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>681879579129200640</td>\n",
       "      <td>positive</td>\n",
       "      <td>Iran ranks st in liver surgeries Allah bless t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>681883903259357184</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Hours before he arrived in Saudi Arabia on Tue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>681904976860327936</td>\n",
       "      <td>negative</td>\n",
       "      <td>Alex Kim Kardashian worth how to love Kim Ka...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>681910549211287552</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I guess even Pandora knows Justin Bieber is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20632 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id sentiment  \\\n",
       "0      619950566786113536   neutral   \n",
       "1      619969366986235905   neutral   \n",
       "2      619971047195045888  negative   \n",
       "3      619974445185302528   neutral   \n",
       "4      619987808317407232  positive   \n",
       "...                   ...       ...   \n",
       "20627  681877834982232064   neutral   \n",
       "20628  681879579129200640  positive   \n",
       "20629  681883903259357184   neutral   \n",
       "20630  681904976860327936  negative   \n",
       "20631  681910549211287552   neutral   \n",
       "\n",
       "                                                   tweet  label  \n",
       "0      Picturehouses Pink Floyds Roger Waters The Wal...      0  \n",
       "1      Order Go Set a Watchman in store or through ou...      0  \n",
       "2      If these runway renovations at the airport pre...     -1  \n",
       "3      If you could ask an onstage interview question...      0  \n",
       "4      A portion of book sales from our Harper LeeGo ...      1  \n",
       "...                                                  ...    ...  \n",
       "20627   from what I think youre asking in no order Fu...      0  \n",
       "20628  Iran ranks st in liver surgeries Allah bless t...      1  \n",
       "20629  Hours before he arrived in Saudi Arabia on Tue...      0  \n",
       "20630    Alex Kim Kardashian worth how to love Kim Ka...     -1  \n",
       "20631  I guess even Pandora knows Justin Bieber is a ...      0  \n",
       "\n",
       "[20632 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processDocument(doc, stemmer): \n",
    "\n",
    "    #Replace @username with empty string\n",
    "    doc = remove_usernames(doc)\n",
    "    #Replace url with empty string\n",
    "    doc = remove_urls(doc)\n",
    "\n",
    "    \n",
    "    #doc = re.sub(r'@[^\\s]+', ' ', doc)\n",
    "    #doc = re.sub(r'_', ' ', doc)\n",
    "    \n",
    "    \n",
    "    doc = re.sub(r'\\n', ' ', doc)\n",
    "    doc = re.sub(r'\\d', '', doc)\n",
    "    #Convert www.* or https?://* to \" \"\n",
    "    doc = re.sub('(www\\.[^\\s])',' ',doc)\n",
    "    #Replace #word with word\n",
    "    doc = re.sub(r'#([^\\s]+)', r'\\1', doc)\n",
    "    \n",
    "    # remove punctuations\n",
    "    doc= remove_punctuations(doc)\n",
    "    # normalize the tweet\n",
    "    #doc= normalize_arabic(doc)\n",
    "    \n",
    "    #Replace numbers with empty string\n",
    "    doc = remove_numbers(doc)\n",
    "    #Replace @username with empty string\n",
    "    doc = remove_hashtags(doc)\n",
    "    # remove repeated letters\n",
    "    #doc=remove_repeating_char(doc)\n",
    "    \n",
    "    #stemming\n",
    "    doc = stemmer.stem(doc)\n",
    "   \n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "df[\"tweet\"] = df['tweet'].apply(lambda x: processDocument(x, stemmer))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "500939eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbabd279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca7e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\luche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bbe90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################trail 1#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb003971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Picturehouses',\n",
       "   'Pink',\n",
       "   'Floyds',\n",
       "   'Roger',\n",
       "   'Waters',\n",
       "   'The',\n",
       "   'Walll',\n",
       "   'opening',\n",
       "   'Sept',\n",
       "   'is',\n",
       "   'now',\n",
       "   'making',\n",
       "   'waves',\n",
       "   'Watch',\n",
       "   'the',\n",
       "   'trailer',\n",
       "   'on',\n",
       "   'Rolling',\n",
       "   'Stone',\n",
       "   'look'],\n",
       "  0),\n",
       " (['Order',\n",
       "   'Go',\n",
       "   'Set',\n",
       "   'a',\n",
       "   'Watchman',\n",
       "   'in',\n",
       "   'store',\n",
       "   'or',\n",
       "   'through',\n",
       "   'our',\n",
       "   'website',\n",
       "   'before',\n",
       "   'Tuesday',\n",
       "   'and',\n",
       "   'get',\n",
       "   'it',\n",
       "   'half',\n",
       "   'price',\n",
       "   'GSAW'],\n",
       "  0),\n",
       " (['If',\n",
       "   'these',\n",
       "   'runway',\n",
       "   'renovations',\n",
       "   'at',\n",
       "   'the',\n",
       "   'airport',\n",
       "   'prevent',\n",
       "   'me',\n",
       "   'from',\n",
       "   'seeing',\n",
       "   'Taylor',\n",
       "   'Swift',\n",
       "   'on',\n",
       "   'Monday',\n",
       "   'Bad',\n",
       "   'Blood',\n",
       "   'will',\n",
       "   'have',\n",
       "   'a',\n",
       "   'new',\n",
       "   'meaning'],\n",
       "  -1),\n",
       " (['If',\n",
       "   'you',\n",
       "   'could',\n",
       "   'ask',\n",
       "   'an',\n",
       "   'onstage',\n",
       "   'interview',\n",
       "   'question',\n",
       "   'at',\n",
       "   'Miss',\n",
       "   'USA',\n",
       "   'tomorrow',\n",
       "   'what',\n",
       "   'would',\n",
       "   'it',\n",
       "   'be'],\n",
       "  0),\n",
       " (['A',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'book',\n",
       "   'sales',\n",
       "   'from',\n",
       "   'our',\n",
       "   'Harper',\n",
       "   'LeeGo',\n",
       "   'Set',\n",
       "   'a',\n",
       "   'Watchman',\n",
       "   'release',\n",
       "   'party',\n",
       "   'on',\n",
       "   'Mon',\n",
       "   'will',\n",
       "   'support',\n",
       "   'and',\n",
       "   'the',\n",
       "   'great',\n",
       "   'work',\n",
       "   'they',\n",
       "   'do'],\n",
       "  1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "# Separating our features (text) and our labels into two lists to smoothen our work\n",
    "X = df['tweet'].tolist()\n",
    "Y = df['label'].tolist()\n",
    "\n",
    "# Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n",
    "# and its corresponding label\n",
    "for x, y in zip(X, Y):\n",
    "    \n",
    "    data.append((nltk.word_tokenize(x), y))\n",
    "    \n",
    "# Printing the CPU time and the first 5 elements of our 'data' list\n",
    "#print('CPU Time:', time() - start_time)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5462c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c679a7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Picturehouses', 'NNS'),\n",
       " ('Pink', 'NNP'),\n",
       " ('Floyds', 'NNP'),\n",
       " ('Roger', 'NNP'),\n",
       " ('Waters', 'NNP'),\n",
       " ('The', 'DT'),\n",
       " ('Walll', 'NNP'),\n",
       " ('opening', 'NN'),\n",
       " ('Sept', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('now', 'RB'),\n",
       " ('making', 'VBG'),\n",
       " ('waves', 'NNS'),\n",
       " ('Watch', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('trailer', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Rolling', 'NNP'),\n",
       " ('Stone', 'NNP'),\n",
       " ('look', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e646ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['picturehouses', 'pink', 'floyds', 'roger', 'water', 'walll', 'opening', 'sept', 'make', 'wave', 'watch', 'trailer', 'rolling', 'stone', 'look']\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = stopwords.words('english')\n",
    "def lemmatize_sentence(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Eliminating the token if it is a link\n",
    "        \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token.lower(), pos)\n",
    "\n",
    "        \n",
    "        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n",
    "        if token not in string.punctuation and len(token) > 2 and token not in STOP_WORDS:\n",
    "            cleaned_tokens.append(token)\n",
    "            \n",
    "    return cleaned_tokens\n",
    "# Prevewing the remove_noise() output\n",
    "print(lemmatize_sentence(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4275befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Noise, CPU Time: 65.58766412734985\n",
      "Data Prepared for model, CPU Time: 0.08128595352172852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'picturehouses': True,\n",
       "   'pink': True,\n",
       "   'floyds': True,\n",
       "   'roger': True,\n",
       "   'water': True,\n",
       "   'walll': True,\n",
       "   'opening': True,\n",
       "   'sept': True,\n",
       "   'make': True,\n",
       "   'wave': True,\n",
       "   'watch': True,\n",
       "   'trailer': True,\n",
       "   'rolling': True,\n",
       "   'stone': True,\n",
       "   'look': True},\n",
       "  0),\n",
       " ({'order': True,\n",
       "   'set': True,\n",
       "   'watchman': True,\n",
       "   'store': True,\n",
       "   'website': True,\n",
       "   'tuesday': True,\n",
       "   'get': True,\n",
       "   'half': True,\n",
       "   'price': True,\n",
       "   'gsaw': True},\n",
       "  0),\n",
       " ({'runway': True,\n",
       "   'renovation': True,\n",
       "   'airport': True,\n",
       "   'prevent': True,\n",
       "   'see': True,\n",
       "   'taylor': True,\n",
       "   'swift': True,\n",
       "   'monday': True,\n",
       "   'bad': True,\n",
       "   'blood': True,\n",
       "   'new': True,\n",
       "   'meaning': True},\n",
       "  -1),\n",
       " ({'could': True,\n",
       "   'ask': True,\n",
       "   'onstage': True,\n",
       "   'interview': True,\n",
       "   'question': True,\n",
       "   'miss': True,\n",
       "   'usa': True,\n",
       "   'tomorrow': True,\n",
       "   'would': True},\n",
       "  0),\n",
       " ({'portion': True,\n",
       "   'book': True,\n",
       "   'sale': True,\n",
       "   'harper': True,\n",
       "   'leego': True,\n",
       "   'set': True,\n",
       "   'watchman': True,\n",
       "   'release': True,\n",
       "   'party': True,\n",
       "   'mon': True,\n",
       "   'support': True,\n",
       "   'great': True,\n",
       "   'work': True},\n",
       "  1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "# As the Naive Bayesian classifier accepts inputs in a dict-like structure,\n",
    "# we have to define a function that transforms our data into the required input structure\n",
    "def list_to_dict(cleaned_tokens):\n",
    "    return dict([token, True] for token in cleaned_tokens)\n",
    "\n",
    "cleaned_tokens_list = []\n",
    "\n",
    "# Removing noise from all the data\n",
    "for tokens, label in data:\n",
    "    cleaned_tokens_list.append((lemmatize_sentence(tokens), label))\n",
    "\n",
    "print('Removed Noise, CPU Time:', time() - start_time)\n",
    "start_time = time()\n",
    "\n",
    "final_data = []\n",
    "\n",
    "# Transforming the data to fit the input structure of the Naive Bayesian classifier\n",
    "for tokens, label in cleaned_tokens_list:\n",
    "    final_data.append((list_to_dict(tokens), label))\n",
    "    \n",
    "print('Data Prepared for model, CPU Time:', time() - start_time)\n",
    "\n",
    "# Previewing our final (tokenized, cleaned and lemmatized) data list\n",
    "final_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1326425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cleaned_tokens_list, open(\"./test.p\" , \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1334eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9061d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './test.p'\n",
    "\n",
    "def load_data(root):\n",
    "    \"\"\"Load data from saved files generated in 'dataset_preprocess.py' \"\"\"\n",
    "    data = pickle.load(open(root, \"rb\"))\n",
    "    return data\n",
    "\n",
    "cleaned_tokens_list = load_data(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fae8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f66ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "495361c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 50-dimensional GloVe embeddings\n",
    "# This method will return three dictionaries:\n",
    "# * word_to_index: a dictionary mapping from words to their indices in the vocabulary\n",
    "# * index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "# * word_to_vec_map: dictionary mapping words to their GloVe vector representation\n",
    "# Note that there are 400,001 words, with the valid indices ranging from 0 to 400,000\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab59a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = np.dot(u, v)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab9ebb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 22\n",
      "[[372306. 285068. 150438. ...      0.      0.      0.]\n",
      " [271173. 325899. 383760. ...      0.      0.      0.]\n",
      " [313470. 305627.  49231. ...      0.      0.      0.]\n",
      " ...\n",
      " [111219. 192758. 230972. ...      0.      0.      0.]\n",
      " [372306. 246253. 374068. ...      0.      0.      0.]\n",
      " [269331. 198755. 389401. ...      0.      0.      0.]]\n",
      "[ 0.  0. -1.  0.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  1.  0.  1.  1.  0.  1.  0. -1.  0.  1.  0.  1.  0. -1.  0.  1.\n",
      "  0.  0.  0. -1. -1.  1.  0.  1. -1.  1.  0. -1. -1.  1.  0.  1. -1. -1.\n",
      "  1.  1.  1.  0. -1.  0.  0.  0. -1.  1.  0.  1.  0.  1.  0.  1.  1.  0.\n",
      "  0.  1. -1. -1.  0. -1.  1.  1.  0. -1.  0.  1.  1. -1. -1. -1.  0.  1.\n",
      "  0.  1.  0.  0.  0.  0.  1.  1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "#start_time = time()\n",
    "\n",
    "unks = []\n",
    "UNKS = []\n",
    "\n",
    "# This function will act as a \"last resort\" in order to try and find the word\n",
    "# in the words embedding layer. It will basically eliminate contiguously occuring\n",
    "# instances of a similar character\n",
    "def cleared(word):\n",
    "    res = \"\"\n",
    "    prev = None\n",
    "    for char in word:\n",
    "        if char == prev: continue\n",
    "        prev = char\n",
    "        res += char\n",
    "    return res\n",
    "\n",
    "\n",
    "def sentence_to_indices(sentence_words, word_to_index, max_len, i):\n",
    "    global X, Y\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            UNKS.append(w)\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "                unks.append(w)\n",
    "        X[i, j] = index\n",
    "\n",
    "\n",
    "# Here we will utilize the already computed 'cleaned_tokens_list' variable\n",
    "   \n",
    "#print('Removed Noise, CPU Time:', time() - start_time)\n",
    "#start_time = time()\n",
    "\n",
    "list_len = [len(i) for i, j in cleaned_tokens_list]\n",
    "max_len = max(list_len)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "X = np.zeros((len(cleaned_tokens_list), max_len))\n",
    "Y = np.zeros((len(cleaned_tokens_list), ))\n",
    "\n",
    "for i, tk_lb in enumerate(cleaned_tokens_list):\n",
    "    tokens, label = tk_lb\n",
    "    sentence_to_indices(tokens, word_to_index, max_len, i)\n",
    "    Y[i] = label\n",
    "    \n",
    "#print('Data Prepared for model, CPU Time:', time() - start_time)\n",
    "\n",
    "\n",
    "print(X[:100])\n",
    "print(Y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32618b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "Y = to_categorical(Y,3)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4081f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(X, open(\"./test_X.p\" , \"wb\"))\n",
    "pickle.dump(Y, open(\"./test_Y.p\" , \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf01a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional,GlobalMaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9729792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 22\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3205e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18529b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 536, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 536, 128)          91648     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 20,092,085\n",
      "Trainable params: 92,035\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\n",
    "\n",
    "# Apply Dropout to use recurrent decimation to combat overfitting \n",
    "model.add(LSTM(128, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)) \n",
    "\n",
    "# Use Flatten() or GlobalMaxPooling1D() to convert 3D output to 2D.\n",
    "# It allows you to add one or more Dense layers to your model \n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "#model.add(Dense(64, activation='relu'))#as experiment we tried to add one additional layer,but it is not give as higher accuracy\n",
    "#model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))#activation function for classification\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955024d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./data/weights.best.1.hdf5')\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "print('Created model and loaded weights from hdf5 file')\n",
    " \n",
    "# estimate\n",
    "scores = model.evaluate(X,Y)\n",
    "print(\"{0}: {1:.2f}%\".format(model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
